import os, asyncio
from emotions.emotions import detect_user_emotions  

BACKEND = os.getenv("BOT_BACKEND", "flan").lower()   # flan | gpt4o-mini
print(f"[backend_factory] BOT_BACKEND = '{BACKEND}'")  # prints flan or gpt4o-mini
# imports depend on the backend
from chatbots.flan_t5 import (
    generate_reflection   as _flan_reflect,
    generate_recommendation as _flan_recommend,
)

from chatbots.GPT4omini import (
    generate_reflection     as _g4o_reflect_async,
    generate_recommendation as _g4o_recommend_async,
)

if BACKEND == "gpt4o-mini":
    def generate_reflection(message: str):
        """
        Detects the user's emotion and generates a reflection response. Uses GPT-4o-mini backend.
        --------
        Args:
            message (str): The user's message to reflect on.
        --------
        Returns:
            str: The reflection response generated by GPT-4o-mini.
        """
        _, emos = detect_user_emotions(message, n=1)
        detected = emos[0] if emos else "sadness"  # Default to sadness if no emotion detected
        return asyncio.run(_g4o_reflect_async(message, detected))

    def generate_recommendation(ui1: str, ui2: str, br1: str, emo1: str, emo2: str):
        """
        Generates a recommendation based on user inputs and emotions. Uses GPT-4o-mini backend.
        --------
        Args:
            ui1 (str): User input 1.
            ui2 (str): User input 2.
            br1 (str): Bot response 1.
            emo1 (str): Current emotion of the user.
            emo2 (str): Desired emotion of the user.
        """
        history = [
            {"role": "user",      "content": ui1},
            {"role": "assistant", "content": br1},
            {"role": "user",      "content": ui2},
        ]
        return asyncio.run(_g4o_recommend_async(
            history,
            current_emotion=emo1,
            desired_emotion=emo2,
        ))

else: # Default to Flan-T5 backend
    def generate_reflection(message: str):
        """
        Generates a reflection response based on the user's message using the Flan-T5 model.
        --------
        Args:
            message (str): The user's message to reflect on.
        --------
        Returns:
            str: The reflection response generated by Flan-T5.
        """
        return _flan_reflect(message)

    def generate_recommendation(ui1: str, ui2: str, br1: str, emo1: str, emo2: str) -> str:
        """
        Generates a recommendation based on user inputs and emotions using the Flan-T5 model.
        --------
        Args:
            ui1 (str): User input 1.
            ui2 (str): User input 2.
            br1 (str): Bot response 1.
            emo1 (str): Current emotion of the user.
            emo2 (str): Desired emotion of the user.
        --------
        Returns:
            str: The recommendation response generated by Flan-T5.
        """
        return _flan_recommend(ui1, ui2, br1, emo1, emo2)